{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4dmztgkea9V"
      },
      "source": [
        "## Mount Google Drive to access datasets and store the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2jYFsJDOIhc",
        "outputId": "4f1d5ad6-d831-4a93-a82d-202dee3b7afc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kle-XcG4ekq2"
      },
      "source": [
        "## Install all the required libraries for sound pre-processing and generating sound from mel spectograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1KtxJc7ZZBr",
        "outputId": "2ee1130d-dd14-4cc0-d8c7-d61a6a7cb2c1"
      },
      "outputs": [],
      "source": [
        "!pip3 install librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRK3ImRAZk00",
        "outputId": "5265db24-f3df-4202-c70b-1188444db3bb"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-gpu==2.3.1\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN3wOe559wjI",
        "outputId": "9ac3a41f-056e-4c16-d5be-d58921f7a2d9"
      },
      "outputs": [],
      "source": [
        "!pip install soundfile\n",
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h6kaZOVdNOf6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv3-5l6SXwWA"
      },
      "source": [
        "## Pre-Processing the Audio\n",
        "\n",
        "- **STEPS** \n",
        "  - 1] Load the File\n",
        "  - 2] Pad Signal (as needed)\n",
        "  - 3] Extract log spectograms from signal\n",
        "  - 4] Normalize Spectogram\n",
        "  - 5] Save normalized spectogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rRnaaA4OXVqn"
      },
      "outputs": [],
      "source": [
        "\"\"\"Loads an audio file\"\"\"\n",
        "class Loader:\n",
        "  def __init__(self, sample_rate, duration, mono):\n",
        "    self.sample_rate = sample_rate\n",
        "    self.duration = duration\n",
        "    self.mono = mono # A mode for processing if false we process as stereo\n",
        "\n",
        "  def load(self, file_path):\n",
        "    signal, sampleRate = librosa.load(path=file_path, \n",
        "                          sr = self.sample_rate,\n",
        "                          mono=self.mono,\n",
        "                          duration = self.duration)\n",
        "    return signal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ktZbn-A3YeVj"
      },
      "outputs": [],
      "source": [
        "\"\"\"Apply padding to an array of signal as needed\"\"\"\n",
        "class Padder:\n",
        "  def __init__(self, mode=\"constant\"):\n",
        "    # constant --> Zero_Padding \n",
        "    self.mode = mode\n",
        "\n",
        "  # ex: [1,2,3] --> zero_padding --> [0,0,1,2,3] padding=2\n",
        "  def left_pad(self, array, num_missing_items):\n",
        "    padded_array = np.pad(array=array,\n",
        "                         #pad_width = (prepend_toarray_num, append_toarray_num) \n",
        "                          pad_width=(num_missing_items, 0),\n",
        "                          mode=self.mode)\n",
        "    return padded_array\n",
        "\n",
        "  \n",
        "  # ex: [1,2,3] --> zero_padding --> [1,2,3,0,0] padding=2\n",
        "  def right_pad(self, array, num_missing_items):\n",
        "    padded_array = np.pad(array=array,\n",
        "                         #pad_width = (prepend_toarray_num, append_toarray_num) \n",
        "                          pad_width=(0, num_missing_items),\n",
        "                          mode=self.mode)\n",
        "    return padded_array    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OpghbCpiYfiC"
      },
      "outputs": [],
      "source": [
        "\"\"\" Extracts log-spectogram in decibels from a time-series signal\"\"\"\n",
        "class LogSpectrogramExtractor:\n",
        "  def __init__(self, frame_size, hop_length):\n",
        "    self.frame_size = frame_size\n",
        "    self.hop_length = hop_length\n",
        "\n",
        "  def extract(self, signal):\n",
        "    # (1 + frame_size / 2, num_frames)--> 2d array\n",
        "    stft = librosa.stft(y = signal,\n",
        "                        n_fft = self.frame_size,\n",
        "                        hop_length = self.hop_length)[:-1] #to get even val from reqd dim\n",
        "\n",
        "    spectrogram = np.abs(stft)\n",
        "    log_spectrogram =librosa.amplitude_to_db(spectrogram)\n",
        "    return log_spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bBvWPJ2cYj1P"
      },
      "outputs": [],
      "source": [
        "\"\"\" Applies Min-Max normalization to an array \"\"\"\n",
        "class MinMaxNormalizer:\n",
        "  def __init__(self, min_val, max_val):\n",
        "    self.min = min_val\n",
        "    self.max = max_val\n",
        "  \n",
        "  def normalize(self, array):\n",
        "    # Squish array between 0 and 1.\n",
        "    norm_arr = (array - array.min()) / (array.max() - array.min())\n",
        "    # Squish array between max and min instead of 0 and 1 norm.\n",
        "    norm_arr = norm_arr * (self.max - self.min) + self.min\n",
        "    return norm_arr\n",
        "\n",
        "  def denormalize(self, norm_arr, og_min_val, og_max_val):\n",
        "    array = (norm_arr - self.min) / (self.max - self.min)\n",
        "    array = array * (og_max_val - og_min_val) + og_min_val\n",
        "    return array\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Vp08qUyUYngX"
      },
      "outputs": [],
      "source": [
        "\"\"\"Save features and min_max values\"\"\"\n",
        "class Saver:\n",
        "  def __init__(self, feature_save_dir, min_max_values_save_dir):\n",
        "        self.feature_save_dir = feature_save_dir\n",
        "        self.min_max_values_save_dir = min_max_values_save_dir\n",
        "\n",
        "  def save_feature(self, feature, file_path):\n",
        "      save_path = self._generate_save_path(file_path)\n",
        "      np.save(save_path, feature)\n",
        "      return save_path\n",
        "\n",
        "  def save_min_max_values(self, min_max_values):\n",
        "      save_path = os.path.join(self.min_max_values_save_dir,\n",
        "                                \"min_max_values.pkl\")\n",
        "      self._save(min_max_values, save_path)\n",
        "\n",
        "  @staticmethod\n",
        "  def _save(data, save_path):\n",
        "      with open(save_path, \"wb\") as f:\n",
        "          pickle.dump(data, f)\n",
        "\n",
        "  def _generate_save_path(self, file_path):\n",
        "      file_name = os.path.split(file_path)[1]\n",
        "      save_path = os.path.join(self.feature_save_dir, file_name + \".npy\")\n",
        "      return save_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ashd5NKuYpIJ"
      },
      "outputs": [],
      "source": [
        "\"\"\" Processes audio files in a directory applying the following steps to the files :\n",
        "  - 1] Load the File\n",
        "  - 2] Pad Signal (as needed)\n",
        "  - 3] Extract log spectograms from signal\n",
        "  - 4] Normalize Spectogram\n",
        "  - 5] Save normalized spectogram\n",
        "\n",
        "  Storing min-max values for all log spectograms (i.e. each audio file)\n",
        "\"\"\"\n",
        "\n",
        "class PreProcessingPipeLine:\n",
        "  def __init__(self):\n",
        "    self._loader = None\n",
        "    self.padder = None\n",
        "    self.extractor = None\n",
        "    self.normaliser = None\n",
        "    self.saver = None\n",
        "    self.min_max_values = {}\n",
        "    self.num_expected_samples = None\n",
        "\n",
        "  @property\n",
        "  def loader(self):\n",
        "    return self._loader\n",
        "\n",
        "  @loader.setter\n",
        "  def loader(self, loader):\n",
        "    self._loader = loader\n",
        "    self.num_expected_samples = int(loader.sample_rate * loader.duration)\n",
        "\n",
        "\n",
        "  def process(self, audio_files_dir):\n",
        "    for root, _, files in os.walk(audio_files_dir):\n",
        "      for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        self._process_file(file_path)\n",
        "        print(f\"Processed file {file_path}\")\n",
        "    self.saver.save_min_max_values(self.min_max_values)\n",
        "\n",
        "  def _process_file(self, file_path):\n",
        "    signal = self.loader.load(file_path)\n",
        "    if self._needs_padding(signal):\n",
        "      signal = self._apply_padding(signal)\n",
        "    feature = self.extractor.extract(signal)\n",
        "    norm_feature =  self.normaliser.normalize(feature)\n",
        "    save_path = self.saver.save_feature(norm_feature, file_path)\n",
        "    self._store_min_max_value(save_path, feature.min(), feature.max())\n",
        "    \n",
        "\n",
        "  def _needs_padding(self, signal):\n",
        "    if len(signal) < self.num_expected_samples:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def _apply_padding(self, signal):\n",
        "    num_missing_samples = self.num_expected_samples - len(signal)\n",
        "    padded_signal = self.padder.right_pad(signal, num_missing_samples)\n",
        "    return padded_signal\n",
        "\n",
        "  def _store_min_max_value(self, save_path, min_val, max_val):\n",
        "    #dictionary within a dictionary\n",
        "    self.min_max_values[save_path] = {\n",
        "        \"min\":min_val,\n",
        "        \"max\":max_val\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNhi86kefM2A"
      },
      "source": [
        "## Define constants and instantiate all objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Cc6BpI_2ei_x"
      },
      "outputs": [],
      "source": [
        "FRAME_SIZE = 512\n",
        "HOP_LENGTH = 256\n",
        "DURATION = 0.74 #seconds\n",
        "SAMPLE_RATE = 22050\n",
        "MONO = True\n",
        "SPECTROGRAMS_SAVE_DIR = \"/content/drive/MyDrive/Audio_Dataset/spectograms_save_dir/\"\n",
        "MIN_MAX_VALUES_SAVE_DIR = \"/content/drive/MyDrive/Audio_Dataset/minmax_vals_save_dir/\"\n",
        "FILES_DIR = \"/content/drive/MyDrive/Audio_Dataset/recordings/\"\n",
        "\n",
        "# instantiate all objects\n",
        "loader = Loader(SAMPLE_RATE, DURATION, MONO)\n",
        "padder = Padder()\n",
        "log_spectrogram_extractor = LogSpectrogramExtractor(FRAME_SIZE, HOP_LENGTH)\n",
        "min_max_normaliser = MinMaxNormalizer(0, 1)\n",
        "saver = Saver(SPECTROGRAMS_SAVE_DIR, MIN_MAX_VALUES_SAVE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPPVTs1-8eIq",
        "outputId": "207ed4c3-3f37-44bd-ae1d-2bfc1e4f334d"
      },
      "outputs": [],
      "source": [
        "preprocessing_pipeline = PreProcessingPipeLine()\n",
        "preprocessing_pipeline.loader = loader\n",
        "preprocessing_pipeline.padder = padder\n",
        "preprocessing_pipeline.extractor = log_spectrogram_extractor\n",
        "preprocessing_pipeline.normaliser = min_max_normaliser\n",
        "preprocessing_pipeline.saver = saver\n",
        "\n",
        "preprocessing_pipeline.process(FILES_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IbFFfikn8kCI"
      },
      "outputs": [],
      "source": [
        "\"\"\" Function to load the Free Sound Digits Dataset into our vae model\"\"\"\n",
        "def load_fsdd(spectograms_path):\n",
        "   x_train = []\n",
        "   file_paths = []\n",
        "   for root, sub_dir, filenames in os.walk(spectograms_path):\n",
        "     for file_name in filenames:\n",
        "       filepath = os.path.join(root, file_name)\n",
        "       spectogram = np.load(file=filepath)\n",
        "       x_train.append(spectogram)\n",
        "       file_paths.append(filepath)\n",
        "   x_train = np.array(x_train) #dim --> (num_bins, num_frames) ---> due to stft\n",
        "   x_train = x_train[..., np.newaxis] # new dim --> (num_samples, n_bins, n_frames, 1) --> needed to process in CNNs which expect 3 dims\n",
        "   return x_train, file_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyQbcVypfiKB"
      },
      "source": [
        "## Functions required to build and save the VAE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6khIJHcBHOlT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend\n",
        "tf.compat.v1.disable_eager_execution() #Eager exectution doesn't work with this VAE, operations can't be calculated before hand here.\n",
        "\n",
        "class VAE:\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_shape,\n",
        "      conv_filters,\n",
        "      conv_kernels,\n",
        "      conv_strides,\n",
        "      latent_dim_space):\n",
        "    # Initial bunch of provided inputs\n",
        "    self.input_shape = input_shape\n",
        "    self.conv_filters = conv_filters\n",
        "    self.conv_kernels = conv_kernels\n",
        "    self.conv_strides = conv_strides\n",
        "    self.latent_dim_space = latent_dim_space\n",
        "    self.recon_loss_weight = 1000000\n",
        "\n",
        "    # Initial state of encoder, decoder, model\n",
        "    self.encoder = None\n",
        "    self.decoder = None\n",
        "    self.model = None\n",
        "    \n",
        "    # Params derived from one's supplied\n",
        "    self.num_conv_layers = len(self.conv_filters)\n",
        "    self.prev_shape = None\n",
        "    \n",
        "    self._model_input = None\n",
        "\n",
        "    self._build()\n",
        "\n",
        "  # Provides the build summary for all components of the VAE\n",
        "  def summary(self):\n",
        "    self.encoder.summary()\n",
        "    self.decoder.summary()\n",
        "    self.model.summary()\n",
        "\n",
        "  # Build all the components of the VAE\n",
        "  def _build(self):\n",
        "    self._build_encoder()\n",
        "    self._build_decoder()\n",
        "    self._build_vae()\n",
        "\n",
        "  # The preprocesssed data needs to be fitted to the model so the model can learn from it.\n",
        "  def train(self, x_train, batch_size, num_epochs):\n",
        "    self.model.fit(x_train,\n",
        "                   x_train,\n",
        "                   batch_size = batch_size,\n",
        "                   epochs = num_epochs,\n",
        "                   shuffle=True)\n",
        "\n",
        "  # Build the VAE\n",
        "  def _build_vae(self):\n",
        "    model_input = self._model_input\n",
        "    model_output = self.decoder(self.encoder(model_input))\n",
        "    self.model = tf.keras.Model(model_input, model_output, name=\"vae\")\n",
        "\n",
        "  # Build vae's Encoder\n",
        "  def _build_encoder(self):\n",
        "    encoderInput = self._add_encoder_input()\n",
        "    conv_layers = self._add_conv_layers(encoderInput)\n",
        "    encoderOutput = self._add_bottleneck(conv_layers)\n",
        "    self._model_input = encoderInput\n",
        "    self.encoder = tf.keras.Model(encoderInput, encoderOutput, name=\"encoder\")\n",
        "\n",
        "  # Add input layer to the encoder\n",
        "  def _add_encoder_input(self):\n",
        "    return tf.keras.layers.Input(shape= self.input_shape, name=\"encoder_input\")\n",
        "\n",
        "  # Prepare a final output layer for vae based on latent_dim\n",
        "  def _add_bottleneck(self, conv_layers):\n",
        "    x = conv_layers\n",
        "    \"\"\"Flatten the data and prepare the final output layer for encoder with Gaussian sampling\"\"\"\n",
        "\n",
        "    self.prev_shape = backend.int_shape(x)[1:] #[batch_size, width, height, Num_channels]\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    \"\"\"Encoding a pt. in latent data_space : z = mu + Summation[epsilon]\"\"\"\n",
        "\n",
        "    # mean\n",
        "    self.mu = tf.keras.layers.Dense(self.latent_dim_space, name=\"mu\")(x)\n",
        "    # log variance\n",
        "    self.log_var = tf.keras.layers.Dense(self.latent_dim_space, name=\"log_var\")(x)\n",
        "\n",
        "    def sample_pt_from_normal_dist(args):\n",
        "      mu, log_var = args\n",
        "      #mean = 0 and stddev = 1--> to get standard normal distribution\n",
        "      epsilon = tf.keras.backend.random_normal(shape=tf.keras.backend.shape(self.mu), mean=0.0, stddev=1.0)\n",
        "      sampled_point = mu + tf.keras.backend.exp(log_var / 2) * (epsilon)\n",
        "      return sampled_point\n",
        "\n",
        "    x = tf.keras.layers.Lambda(sample_pt_from_normal_dist, name=\"Encoder_Output\")([self.mu, self.log_var])\n",
        "    return x\n",
        "\n",
        "  # Create all conv blocks for encoder based on num_conv_layers requested\n",
        "  def _add_conv_layers(self, encoderInput):\n",
        "    x = encoderInput\n",
        "    for layer_idx in range(self.num_conv_layers):\n",
        "      x = self._add_conv_layer(layer_idx, x)\n",
        "    return x\n",
        "\n",
        "    \"\"\"Adds a conv block to a graph of layers which contain \n",
        "  multiple conv2d + ReLU + batch_normalizaton\"\"\"\n",
        "  def _add_conv_layer(self, layer_idx, x):\n",
        "    conv_layer = tf.keras.layers.Conv2D(\n",
        "        filters = self.conv_filters[layer_idx],\n",
        "        kernel_size = self.conv_kernels[layer_idx],\n",
        "        strides = self.conv_strides[layer_idx],\n",
        "        padding = \"same\",\n",
        "        name=f\"Encoder_Conv_Layer-{layer_idx+1}\",\n",
        "    )\n",
        "\n",
        "    x = conv_layer(x)\n",
        "    x = tf.keras.layers.ReLU(name=f\"Encoder_ReLU_{layer_idx + 1}\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization(name=f\"Encoder_BatchNormalization_{layer_idx+1}\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  \n",
        "  # Build vae's decoder\n",
        "  def _build_decoder(self):\n",
        "    decoderInput = self._add_decoder_input()\n",
        "    dense_layer = self._add_dense_layer(decoderInput)\n",
        "    reshape_layer = self._add_reshape_layer(dense_layer)\n",
        "    conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
        "    decoderOutput = self._add_decoder_output(conv_transpose_layers)\n",
        "    self.decoder = tf.keras.Model(decoderInput, decoderOutput, name=\"decoder\")\n",
        "\n",
        "  # Add input layer to the decoder\n",
        "  def _add_decoder_input(self):\n",
        "    return tf.keras.layers.Input(shape=self.latent_dim_space, name=\"decoder_input\")\n",
        "\n",
        "  # Add dense layer to the decoder\n",
        "  def _add_dense_layer(self, decoderInput):\n",
        "    num_units = np.prod(self.prev_shape) #prev_shape = [x,y,z] -->prod = x*y*z\n",
        "    return tf.keras.layers.Dense(units=num_units, name=\"decoder_dense_layer\")(decoderInput)\n",
        "\n",
        "  # Reshape the layer to match target shape needed by decoder\n",
        "  def _add_reshape_layer(self, dense_layer):\n",
        "    reshape_layer = tf.keras.layers.Reshape(target_shape=self.prev_shape)(dense_layer)\n",
        "    return reshape_layer\n",
        "\n",
        "  # Add convolution transpose block to re-create image from latent dims.\n",
        "  def _add_conv_transpose_layers(self, x):\n",
        "    \"\"\"Add convolutional transpose blocks\"\"\"\n",
        "    # We need to ignore first conv layer we are going in reverse order of conv_layers used\n",
        "    # in encoder.\n",
        "    for layer_idx in reversed(range(1,self.num_conv_layers)):\n",
        "      x = self._add_conv_transpose_layer(layer_idx, x)\n",
        "    return x\n",
        "\n",
        "  # Add specific conv_transpose_layer in a specific position\n",
        "  def _add_conv_transpose_layer(self, layer_idx, x):\n",
        "    layer_num = self.num_conv_layers - layer_idx\n",
        "\n",
        "    conv_trans_layer = tf.keras.layers.Conv2DTranspose(\n",
        "        filters = self.conv_filters[layer_idx],\n",
        "        kernel_size = self.conv_kernels[layer_idx],\n",
        "        strides = self.conv_strides[layer_idx],\n",
        "        padding = \"same\",\n",
        "        name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
        "    )\n",
        "\n",
        "    x = conv_trans_layer(x)\n",
        "    x = tf.keras.layers.ReLU(name=f\"decoder_ReLU_{layer_num}\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization(name=f\"decoder_batchnormalization_{layer_num}\")(x)\n",
        "    return x\n",
        "\n",
        "  # Add final output layer to the decoder\n",
        "  def _add_decoder_output(self, x):\n",
        "    conv_trans_layer = tf.keras.layers.Conv2DTranspose(\n",
        "        filters = self.conv_filters[0],\n",
        "        kernel_size = self.conv_kernels[0],\n",
        "        strides = self.conv_strides[0],\n",
        "        padding=\"same\",\n",
        "        name=f\"decoder_final_conv_trans_layer_{self.num_conv_layers}\",\n",
        "    )\n",
        "\n",
        "    x = conv_trans_layer(x)\n",
        "    outputLayer = tf.keras.layers.Activation(\"sigmoid\", name=f\"decoder_output_layer\")(x)\n",
        "    return outputLayer\n",
        "\n",
        "  # Compile the model\n",
        "  def compile(self, learning_rate=0.0001):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    self.model.compile(optimizer = optimizer, \n",
        "                       loss = self._calculate_combined_loss, \n",
        "                       metrics=[self._calculate_reconstruction_loss,\n",
        "                                self._calculate_kl_loss])\n",
        "\n",
        "  # Reconstruction Loss\n",
        "  def _calculate_reconstruction_loss(self, y_target, y_pred):\n",
        "    error = y_target - y_pred\n",
        "    recon_loss = tf.keras.backend.mean(tf.keras.backend.square(error), axis=[1,2,3])\n",
        "    return recon_loss\n",
        "\n",
        "  # Kullback-Libler Loss\n",
        "  def _calculate_kl_loss(self, y_target, y_pred):\n",
        "    # kl_loss = 1/2 * (Summation[ 1 + log(variance) - mean^2 - variance^2]), - -->sign used because we are calculating loss\n",
        "    kl_loss = - 0.5 * tf.keras.backend.sum(1 + self.log_var - tf.keras.backend.square(self.mu) - \n",
        "                                           tf.keras.backend.exp(self.log_var), \n",
        "                                           axis=1)\n",
        "    return kl_loss\n",
        "\n",
        "  def _calculate_combined_loss(self, y_target, y_pred):\n",
        "    recon_loss = self._calculate_reconstruction_loss(y_target, y_pred)\n",
        "    kl_loss = self._calculate_kl_loss(y_target, y_pred)\n",
        "    combined_loss = self.recon_loss_weight * recon_loss + kl_loss\n",
        "\n",
        "    return combined_loss\n",
        "\n",
        "\n",
        "\n",
        "  # Save the model\n",
        "  def save(self, save_folder=\".\"):\n",
        "    self._create_save_dir(save_folder)\n",
        "    self._save_params(save_folder)\n",
        "    self._save_weights(save_folder)\n",
        "  \n",
        "  def _create_save_dir(self, sfolder):\n",
        "    if not os.path.exists(sfolder):\n",
        "      os.makedirs(sfolder)\n",
        "\n",
        "  # Params to be saved\n",
        "  def _save_params(self, sfolder):\n",
        "    params = [self.input_shape,    # Initial input model will get\n",
        "              self.conv_filters,  # List containing num of filters for each layer\n",
        "              self.conv_kernels, # List containing num of kernels for each layer\n",
        "              self.conv_strides,  # List containing strides for each layer\n",
        "              self.latent_dim_space,\n",
        "              ]\n",
        "    save_path = os.path.join(sfolder, \"params.pkl\")\n",
        "    with open(save_path, \"wb\")  as f:\n",
        "      pickle.dump(params, f)\n",
        "\n",
        "  # Weights to be saved\n",
        "  def _save_weights(self, sfolder):\n",
        "     save_path = os.path.join(sfolder, \"weights.h5\")\n",
        "     self.model.save_weights(save_path)\n",
        "\n",
        "  def load(cls, sfolder=\".\"):\n",
        "    param_path = os.path.join(sfolder, \"params.pkl\")\n",
        "    with open(param_path, \"rb\") as f:\n",
        "      params = pickle.load(f)\n",
        "    vae = VAE(*params)\n",
        "    weights_path = os.path.join(sfolder, \"weights.h5\")\n",
        "    vae.load_weights(weights_path)\n",
        "    return vae\n",
        "\n",
        "  def load_weights(self, weights_path):\n",
        "    self.model.load_weights(weights_path)\n",
        "\n",
        "  def reconstruct(self, images):\n",
        "      latent_representations = self.encoder.predict(images)\n",
        "      reconstructed_images = self.decoder.predict(latent_representations)\n",
        "      return reconstructed_images, latent_representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fa6FYvwMHy5E"
      },
      "outputs": [],
      "source": [
        "training_set, _= load_fsdd(\"/content/drive/MyDrive/Audio_Dataset/spectograms_save_dir\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMsBUd7_fx0k"
      },
      "source": [
        "## Prepare the VAE for handling sound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xtpRDuGDIJPz"
      },
      "outputs": [],
      "source": [
        "soundVAE = VAE(\n",
        "    input_shape=(256, 64, 1),\n",
        "    conv_filters=(512,256,128,64,32),\n",
        "    conv_kernels=(3,3,3,3,3),\n",
        "    conv_strides=(2,2,2,2,(2,1)),\n",
        "    latent_dim_space=128,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8920EuCAydY"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 200\n",
        "\n",
        "soundVAE.summary()\n",
        "soundVAE.compile(LEARNING_RATE)\n",
        "soundVAE.train(x_train=training_set, batch_size=BATCH_SIZE, num_epochs=EPOCHS)\n",
        "soundVAE.save('model')\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1oIj3oRYJWPK"
      },
      "outputs": [],
      "source": [
        "from contextlib import redirect_stdout\n",
        "\n",
        "with open('modelsummary.txt', 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        soundVAE.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro2t56yqf_rS"
      },
      "source": [
        "## Generate Sound from Mel Spectograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QLjuXZFLJIU6"
      },
      "outputs": [],
      "source": [
        "\"\"\"Responsible for generating audio from spectograms\"\"\"\n",
        "class SoundGenerator:\n",
        "  def __init__(self, vae, hop_length):\n",
        "    self.vae = vae\n",
        "    self.hop_length = hop_length\n",
        "    self._min_max_normalizer =  MinMaxNormalizer(0,1)\n",
        "\n",
        "  def generator(self, spectograms, min_max_values):\n",
        "    generated_spectograms, latent_reps = self.vae.reconstruct(spectograms)\n",
        "    signals = self.convert_spec_to_audio(generated_spectograms, min_max_values)\n",
        "    return signals, latent_reps\n",
        "\n",
        "  def convert_spec_to_audio(self, spectrograms, min_max_values):\n",
        "    signals = []\n",
        "    for spec, values in zip(spectrograms, min_max_values):\n",
        "      #Reshape log spectogram\n",
        "      log_spectogram = spec[:, :, 0] #Copy 1st and 2nd dim drop 3rd one.\n",
        "      #Apply denormalization\n",
        "      denorm_log_spec = self._min_max_normalizer.denormalize(\n",
        "          log_spectogram, values[\"min\"], values[\"max\"]\n",
        "      )\n",
        "      #Log spectogram --> Linear Spectogram\n",
        "      spectogram = librosa.db_to_amplitude(denorm_log_spec) #Decibels --> Amplitude\n",
        "      #Apply inverse stft (griffin-lim algo)\n",
        "      audio = librosa.istft(spectogram, hop_length=self.hop_length)\n",
        "      signals.append(audio)\n",
        "    return signals\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "f8yBr1tf93nx"
      },
      "outputs": [],
      "source": [
        "HOP_LENGTH = 256\n",
        "SAVE_DIR_OG = \"/content/drive/MyDrive/Sound_DIR/OG_SOUNDS\"\n",
        "SAVE_DIR_GEN = \"/content/drive/MyDrive/Sound_DIR/GEN_SOUNDS\"\n",
        "MIN_MAX_VALUES_PATH = \"/content/drive/MyDrive/Audio_Dataset/minmax_vals_save_dir/min_max_values.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "enOBdESH-bGC"
      },
      "outputs": [],
      "source": [
        "def select_spectrograms(spectrograms,\n",
        "                        file_paths,\n",
        "                        min_max_values,\n",
        "                        num_spectrograms=2):\n",
        "    sampled_indexes = np.random.choice(range(len(spectrograms)), num_spectrograms)\n",
        "    sampled_spectrogrmas = spectrograms[sampled_indexes]\n",
        "    file_paths = [file_paths[index] for index in sampled_indexes]\n",
        "    sampled_min_max_values = [min_max_values[file_path] for file_path in\n",
        "                           file_paths]\n",
        "    print(file_paths)\n",
        "    print(sampled_min_max_values)\n",
        "    return sampled_spectrogrmas, sampled_min_max_values\n",
        "\n",
        "\n",
        "\n",
        "def save_signals(signals, save_dir, sample_rate=22050):\n",
        "    for i, signal in enumerate(signals):\n",
        "        save_path = os.path.join(save_dir, str(i) + \".wav\")\n",
        "        sf.write(save_path, signal, sample_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_wnlcbP_pvp",
        "outputId": "21601460-3b63-4f79-80ab-17f1e3ff02df"
      },
      "outputs": [],
      "source": [
        "# initialise sound generator\n",
        "svae = soundVAE.load(\"/content/drive/MyDrive/Trained_Sound_VAE\")\n",
        "soundGen = SoundGenerator(svae, HOP_LENGTH)\n",
        "\n",
        "# load spectrograms + min max values\n",
        "with open(MIN_MAX_VALUES_PATH, \"rb\") as f:\n",
        "    min_max_values = pickle.load(f)\n",
        "\n",
        "specs, file_paths = load_fsdd(\"/content/drive/MyDrive/Audio_Dataset/spectograms_save_dir\")\n",
        "\n",
        "# sample spectrograms + min max values\n",
        "sampled_specs, sampled_min_max_values = select_spectrograms(specs,\n",
        "                                                            file_paths,\n",
        "                                                            min_max_values,\n",
        "                                                            5)\n",
        "\n",
        "# generate audio for sampled spectrograms\n",
        "signals, _ = soundGen.generator(sampled_specs,\n",
        "                                      sampled_min_max_values)\n",
        "\n",
        "# convert spectrogram samples to audio\n",
        "original_signals = soundGen.convert_spec_to_audio(\n",
        "    sampled_specs, sampled_min_max_values)\n",
        "\n",
        "# save audio signals\n",
        "save_signals(signals, SAVE_DIR_GEN)\n",
        "save_signals(original_signals, SAVE_DIR_OG)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Sound_VAE.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
